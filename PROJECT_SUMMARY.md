# Meeting Engagement Monitor - Project Summary

## Overview

A real-time sentiment analysis application that monitors engagement levels in corporate meetings by analyzing live webcam footage and audio. The system uses multiple computer vision and audio processing techniques to provide comprehensive engagement metrics.

## Core Features

### ğŸ¥ Visual Analysis
- **Multi-Person Detection**: YOLO v8 detects and tracks up to 20 individuals simultaneously
- **Facial Emotion Recognition**: DeepFace analyzes 7 emotions (happy, sad, angry, surprise, fear, disgust, neutral)
- **Body Language Analysis**: MediaPipe tracks 33 body landmarks to detect:
  - Arms crossed (defensive posture)
  - Arms raised (active participation)
  - Forward/backward lean (engagement indicators)
  - Overall posture assessment

### ğŸ¤ Audio Analysis
- **Speech-to-Text**: Whisper transcribes room audio in real-time
- **Sentiment Analysis**: VADER analyzes text sentiment (positive/negative/neutral)
- **Participation Tracking**: Monitors who is speaking and for how long
- **Keyword Extraction**: Identifies frequently mentioned topics

### ğŸ“Š Engagement Scoring

**Individual Person Metrics** (0-100 scale):
- **Emotion Score** (30%): Positive facial expressions boost engagement
- **Body Language Score** (25%): Open posture, forward lean indicate engagement
- **Gaze Score** (15%): Looking at screen/presenter
- **Micro-Expression Score** (10%): Subtle emotional indicators
- **Movement Score** (10%): Appropriate gestures vs fidgeting
- **Speech Score** (10%): Active participation and sentiment

**Room-Level Metrics**:
- Overall engagement average
- Distribution (highly engaged / neutral / disengaged)
- Total and active participants
- Speaking rate and participation percentage
- Sentiment trends over time

### ğŸ–¥ï¸ Real-Time Dashboard

- **Live Engagement Score**: Overall room engagement with visual indicator
- **Participant Cards**: Individual tracking with emotion, posture, and component scores
- **Distribution Chart**: Visual breakdown of engagement levels
- **Metrics Overview**: Quick stats on participants, speakers, and participation rate
- **Near Real-Time Updates**: Metrics update every 5-10 seconds

## Technology Stack

### Backend (Python)
- **FastAPI**: Modern async web framework
- **Ultralytics (YOLO v8)**: State-of-the-art object detection
- **MediaPipe**: Google's pose estimation solution
- **DeepFace**: Facial emotion recognition
- **Whisper (OpenAI)**: Speech-to-text transcription
- **VADER**: Sentiment analysis
- **OpenCV**: Video processing
- **WebSockets**: Real-time bidirectional communication

### Frontend (TypeScript/React)
- **React 19**: Modern UI framework
- **TypeScript**: Type-safe development
- **TailwindCSS**: Utility-first styling
- **Vite**: Fast build tool
- **WebRTC**: Camera and microphone access

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser Client â”‚
â”‚   (React App)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WebSocket
         â”‚ Video Frames
         â”‚ Audio Chunks
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       FastAPI Backend               â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Video Processing Pipeline  â”‚  â”‚
â”‚  â”‚                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ YOLO â”‚â†’ â”‚   Tracker  â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚       â†“                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚MediaPipe â”‚  â”‚DeepFace â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Audio Processing Pipeline  â”‚  â”‚
â”‚  â”‚                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Whisper â”‚â†’ â”‚  VADER   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Engagement Scorer          â”‚  â”‚
â”‚  â”‚   (Weighted Aggregation)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Engagement Metrics
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Live Dashboard â”‚
â”‚  (React UI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
sentiment-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py              # REST API endpoints
â”‚   â”‚   â”œâ”€â”€ websocket_manager.py   # WebSocket connections
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ person_detector.py     # YOLO detection
â”‚   â”‚   â”œâ”€â”€ person_tracker.py      # Multi-object tracking
â”‚   â”‚   â”œâ”€â”€ emotion_analyzer.py    # DeepFace emotions
â”‚   â”‚   â”œâ”€â”€ pose_estimator.py      # MediaPipe poses
â”‚   â”‚   â”œâ”€â”€ speech_analyzer.py     # Whisper + VADER
â”‚   â”‚   â”œâ”€â”€ engagement_scorer.py   # Scoring algorithm
â”‚   â”‚   â”œâ”€â”€ video_processor.py     # Main pipeline
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                    # Model storage
â”‚   â”œâ”€â”€ config.py                  # Configuration
â”‚   â”œâ”€â”€ main.py                    # Entry point
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EngagementScore.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MetricsOverview.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DistributionChart.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ParticipantsList.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioCapture.tsx
â”‚   â”‚   â”‚   â””â”€â”€ VideoCapture.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts
â”‚   â”‚   â”œâ”€â”€ types.ts
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ main.tsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ README.md
â”œâ”€â”€ SETUP_GUIDE.md
â””â”€â”€ PROJECT_SUMMARY.md (this file)
```

## Key Algorithms

### Person Tracking (IOU-based)
1. Detect persons in current frame (YOLO)
2. Compute IoU between existing tracks and new detections
3. Use Hungarian algorithm for optimal assignment
4. Create new tracks for unmatched detections
5. Remove stale tracks (not seen for 30 frames)

### Engagement Scoring
```python
engagement_score = (
    emotion_score * 0.30 +
    body_language_score * 0.25 +
    gaze_score * 0.15 +
    micro_expression_score * 0.10 +
    movement_score * 0.10 +
    speech_score * 0.10
)
```

Categories:
- **Highly Engaged**: Score â‰¥ 0.7 (70%)
- **Neutral**: 0.4 â‰¤ Score < 0.7 (40-69%)
- **Disengaged**: Score < 0.4 (<40%)

### Posture Analysis
- **Forward Lean**: Nose z-coordinate < shoulder z-coordinate â†’ High engagement
- **Arms Crossed**: Wrists crossed body midline â†’ Low engagement
- **Arms Raised**: Wrist y-coordinate < shoulder y-coordinate â†’ Active participation

## Performance Characteristics

### Model Sizes
- **YOLO v8 Nano**: ~6 MB, ~5ms inference on GPU
- **MediaPipe Pose**: ~5 MB, ~10ms inference
- **DeepFace**: ~100 MB, ~50ms per face
- **Whisper Base**: ~140 MB, ~1-2s per 30s audio

### Processing Speed
- **Frame Analysis**: 0.5-2 seconds (depends on person count)
- **Update Interval**: 5-10 seconds (configurable)
- **Latency**: Near real-time (5-7 second total delay)

### Resource Usage
- **CPU**: Moderate (can run on modern laptop i5/i7)
- **RAM**: 2-4 GB (models + processing)
- **GPU**: Optional (2-5x speedup with NVIDIA GPU)

## Use Cases

### Primary: Corporate Meetings
- Monitor audience engagement during presentations
- Identify when to adjust presentation style
- Track overall meeting effectiveness
- Generate post-meeting engagement reports

### Other Applications
- **Education**: Track student engagement in classrooms
- **Virtual Events**: Monitor online webinar participation
- **UX Research**: Study user reactions to products
- **Training**: Assess trainee attention during sessions

## Future Enhancements

### Planned Features
1. **FastVLM Integration**: Add contextual visual understanding
2. **Speaker Diarization**: Identify individual speakers
3. **Gaze Detection**: Track where people are looking
4. **Historical Analytics**: Compare sessions over time
5. **Alerts**: Notify when engagement drops
6. **Recording Playback**: Review past sessions
7. **Multi-Camera Support**: Cover larger rooms
8. **Export Reports**: PDF/CSV engagement summaries

### Potential Improvements
- [ ] Add micro-expression detection for subtle cues
- [ ] Implement fidgeting/movement detection
- [ ] Add head pose estimation for gaze approximation
- [ ] Support for multiple rooms simultaneously
- [ ] Mobile app version
- [ ] Integration with calendar apps (Google/Outlook)
- [ ] Machine learning to predict engagement trends
- [ ] Custom alerting rules

## Limitations & Considerations

### Technical Limitations
- Requires good lighting for accurate face detection
- Audio quality affects transcription accuracy
- CPU-only mode is slower (GPU recommended)
- Limited to ~20 persons for real-time processing
- Needs stable internet for model downloads

### Privacy Considerations
- All processing can be done locally (no cloud required)
- Video/audio not stored by default
- Consider consent requirements in your jurisdiction
- Recommend transparent usage policies
- Option to disable recording features

### Accuracy Factors
- Emotion recognition ~70-80% accurate
- Pose estimation highly accurate (>90%)
- Speech transcription ~80-95% (depends on audio quality)
- Overall engagement score is heuristic-based

## Installation Quick Start

```bash
# Backend
cd backend
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
cp .env.example .env
python main.py

# Frontend (in new terminal)
cd frontend
npm install
npm run dev
```

Visit `http://localhost:5173` and allow camera/microphone permissions.

## Configuration

Key environment variables (`.env`):
```bash
# Processing
FRAME_PROCESSING_INTERVAL=5        # Seconds between frame analysis
MAX_PERSONS_TO_TRACK=20            # Maximum persons to track

# Model Selection
WHISPER_MODEL_SIZE=base            # tiny, base, small, medium, large
YOLO_MODEL_PATH=models/yolov8n.pt

# Engagement Weights (emotion, body, gaze, micro, movement, speech)
ENGAGEMENT_SCORE_WEIGHTS=0.3,0.25,0.15,0.10,0.10,0.10

# Feature Toggles
ENABLE_FACE_DETECTION=true
ENABLE_POSE_ESTIMATION=true
ENABLE_EMOTION_RECOGNITION=true
ENABLE_SPEECH_TO_TEXT=true
```

## Contribution Areas

Potential areas for contribution:
1. Add gaze detection implementation
2. Implement micro-expression analysis
3. Add FastVLM integration
4. Improve tracking algorithm
5. Create post-session report generator
6. Add database persistence layer
7. Implement speaker diarization
8. Create mobile app version
9. Add unit and integration tests
10. Improve documentation

## License

MIT License - See LICENSE file for details

## Credits

### Libraries & Models
- **YOLO v8**: Ultralytics
- **MediaPipe**: Google
- **DeepFace**: Serengil & Ozpinar
- **Whisper**: OpenAI
- **VADER**: Hutto & Gilbert
- **FastAPI**: SebastiÃ¡n RamÃ­rez
- **React**: Meta/Facebook

### Inspired By
- FastVLM WebGPU demo (Apple)
- Real-time emotion detection research
- Meeting engagement studies

## Contact & Support

For questions, issues, or contributions:
- Check SETUP_GUIDE.md for troubleshooting
- Review API documentation at `/docs`
- See README.md for quick reference

---

**Built for analyzing engagement in corporate meetings and presentations.**

*Note: This is a research/demo application. For production use, ensure compliance with privacy laws and obtain proper user consent.*
